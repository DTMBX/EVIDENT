"""
Batch PDF Processing Module
Phase 9 & 10: Document ingestion, OCR, context extraction
"""

from sqlalchemy import Column, Integer, String, Text, Float, JSON, DateTime, Boolean, ForeignKey
from sqlalchemy.orm import relationship
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime
import logging

Base = declarative_base()
logger = logging.getLogger(__name__)

# ============================================================================
# DATABASE MODELS
# ============================================================================

class ProjectDocument(Base):
    """Document uploaded to project"""
    __tablename__ = "project_documents"
    
    id = Column(Integer, primary_key=True)
    project_id = Column(Integer, ForeignKey("projects.id"), nullable=False)
    
    filename = Column(String(255), nullable=False)
    original_filename = Column(String(255), nullable=False)
    file_type = Column(String(50), nullable=False)  # "pdf", "docx", "txt"
    file_size_bytes = Column(Integer, nullable=False)
    file_hash_sha256 = Column(String(64), unique=True, nullable=False)  # For dedup
    
    # File storage
    file_path = Column(Text, nullable=False)  # S3 path or local path
    storage_location = Column(String(50), nullable=False)  # "s3", "local", "gcs"
    
    # Processing status
    processing_status = Column(String(50), default="uploaded")
    # Status: "uploaded" → "queued" → "processing" → "extracting" → "complete" / "failed"
    
    ocr_applied = Column(Boolean, default=False)
    ocr_confidence = Column(Float, default=0.0)  # 0-1 confidence score
    
    text_extracted = Column(Text, nullable=True)  # Full extracted text
    text_length = Column(Integer, default=0)
    
    page_count = Column(Integer, nullable=True)
    
    # Metadata
    document_type = Column(String(50), nullable=True)  # "complaint", "discovery", "motion", etc.
    tags = Column(JSON, default=[])  # ["urgent", "discovery", "motion"]
    
    # Context extracted
    context_extracted = Column(JSON, nullable=True)  # {"parties": [...], "case_number": "..."}
    
    # Processing timestamps
    uploaded_at = Column(DateTime, default=datetime.utcnow)
    processing_started_at = Column(DateTime, nullable=True)
    processing_completed_at = Column(DateTime, nullable=True)
    
    # Relationships
    project = relationship("Project", back_populates="documents")

class DocumentContext(Base):
    """Extracted context from document"""
    __tablename__ = "document_context"
    
    id = Column(Integer, primary_key=True)
    document_id = Column(Integer, ForeignKey("project_documents.id"), nullable=False)
    
    # Parties
    case_number = Column(String(50), nullable=True)
    plaintiff = Column(String(255), nullable=True)
    defendant = Column(String(255), nullable=True)
    judge = Column(String(255), nullable=True)
    court = Column(String(255), nullable=True)
    
    # Violations detected
    violations = Column(JSON, default=[])  # List of violation types found
    
    # Key dates
    filing_date = Column(DateTime, nullable=True)
    hearing_date = Column(DateTime, nullable=True)
    deadline = Column(DateTime, nullable=True)
    
    # Relief/damages
    relief_requested = Column(Text, nullable=True)
    damages_claimed = Column(String(100), nullable=True)
    
    # Document classification
    document_nature = Column(String(100), nullable=True)  # "complaint", "response", "motion", etc
    
    # Confidence in extraction
    extraction_confidence = Column(Float, default=0.0)
    
    created_at = Column(DateTime, default=datetime.utcnow)

class DocumentChainOfCustody(Base):
    """Track document processing and handling"""
    __tablename__ = "document_chain_of_custody"
    
    id = Column(Integer, primary_key=True)
    document_id = Column(Integer, ForeignKey("project_documents.id"), nullable=False)
    
    original_hash = Column(String(64), nullable=False)  # SHA-256
    processing_step = Column(String(100), nullable=False)  # "uploaded", "ocr", "extraction", etc
    
    hash_after_processing = Column(String(64), nullable=False)
    integrity_verified = Column(Boolean, default=True)
    
    processor = Column(String(100), nullable=False)  # "tesseract-v5", "easyocr", "user"
    processed_by_user_id = Column(Integer, nullable=True)
    
    timestamp = Column(DateTime, default=datetime.utcnow)
    
    notes = Column(Text, nullable=True)

class BatchProcessingJob(Base):
    """Track batch processing jobs"""
    __tablename__ = "batch_processing_jobs"
    
    id = Column(Integer, primary_key=True)
    project_id = Column(Integer, ForeignKey("projects.id"), nullable=False)
    
    batch_id = Column(String(50), unique=True, nullable=False)  # "batch_abc123"
    
    status = Column(String(50), default="queued")  # "queued", "processing", "complete", "failed"
    
    total_documents = Column(Integer, default=0)
    processed_documents = Column(Integer, default=0)
    failed_documents = Column(Integer, default=0)
    
    estimated_completion_time = Column(DateTime, nullable=True)
    
    # Results summary
    total_pages = Column(Integer, default=0)
    total_text_extracted = Column(Integer, default=0)
    average_ocr_confidence = Column(Float, default=0.0)
    
    errors = Column(JSON, default=[])  # List of errors
    
    created_at = Column(DateTime, default=datetime.utcnow)
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)

# ============================================================================
# SERVICE CLASSES
# ============================================================================

class PDFBatchLoader:
    """Load multiple PDFs concurrently with error handling"""
    
    def __init__(self, max_concurrent: int = 5, skip_errors: bool = True):
        self.max_concurrent = max_concurrent
        self.skip_errors = skip_errors
        self.logger = logging.getLogger(__name__)
    
    async def load_batch(self, pdf_files: list) -> list:
        """
        Load 10-25 PDFs concurrently
        
        Args:
            pdf_files: List of file paths
        
        Returns:
            [{
                'file_path': str,
                'status': 'loaded' | 'error',
                'page_count': int,
                'file_size_bytes': int,
                'error_message': str (optional)
            }]
        """
        import asyncio
        from concurrent.futures import ThreadPoolExecutor
        
        results = []
        
        # Create semaphore to limit concurrent loads
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def load_with_semaphore(file_path):
            async with semaphore:
                return await self._load_single_pdf(file_path)
        
        # Load all PDFs concurrently
        tasks = [load_with_semaphore(f) for f in pdf_files]
        results = await asyncio.gather(*tasks, return_exceptions=self.skip_errors)
        
        return results
    
    async def _load_single_pdf(self, file_path: str) -> dict:
        """Load single PDF with error handling"""
        try:
            import PyPDF2
            import os
            
            if not os.path.exists(file_path):
                return {
                    'file_path': file_path,
                    'status': 'error',
                    'error_message': f'File not found: {file_path}'
                }
            
            # Open and validate PDF
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                page_count = len(pdf_reader.pages)
            
            file_size = os.path.getsize(file_path)
            
            return {
                'file_path': file_path,
                'status': 'loaded',
                'page_count': page_count,
                'file_size_bytes': file_size
            }
        
        except Exception as e:
            self.logger.error(f"Error loading PDF {file_path}: {str(e)}")
            return {
                'file_path': file_path,
                'status': 'error',
                'error_message': str(e)
            }

class OCREngine:
    """Extract text from PDFs using multiple OCR engines"""
    
    def __init__(self, model: str = 'tesseract-v5', fallback: str = 'easyocr'):
        """
        Args:
            model: Primary OCR model ('tesseract-v5', 'easyocr', 'paddleocr')
            fallback: Fallback model if primary fails
        """
        self.model = model
        self.fallback = fallback
        self.logger = logging.getLogger(__name__)
    
    async def extract_text(self, pdf_path: str) -> dict:
        """
        Extract text from scanned PDF with high accuracy
        
        Returns:
            {
                'text': str,  # Full extracted text
                'page_count': int,
                'confidence': float,  # Overall confidence 0-1
                'per_page_confidence': [float],  # Confidence per page
                'processing_time_seconds': float,
                'model_used': str
            }
        """
        import time
        
        start_time = time.time()
        
        try:
            if self.model == 'tesseract-v5':
                result = await self._extract_tesseract(pdf_path)
            elif self.model == 'easyocr':
                result = await self._extract_easyocr(pdf_path)
            else:
                result = await self._extract_tesseract(pdf_path)
            
            result['processing_time_seconds'] = time.time() - start_time
            return result
        
        except Exception as e:
            self.logger.warning(f"Primary OCR failed, trying fallback: {str(e)}")
            
            # Try fallback
            if self.fallback != self.model:
                if self.fallback == 'easyocr':
                    result = await self._extract_easyocr(pdf_path)
                else:
                    result = await self._extract_tesseract(pdf_path)
                
                result['processing_time_seconds'] = time.time() - start_time
                return result
            
            raise
    
    async def _extract_tesseract(self, pdf_path: str) -> dict:
        """Extract using Tesseract OCR (95%+ accuracy)"""
        import pytesseract
        from pdf2image import convert_from_path
        
        # Convert PDF to images
        pages = convert_from_path(pdf_path)
        
        full_text = []
        per_page_confidence = []
        
        for page in pages:
            # Preprocess image for better OCR
            import cv2
            import numpy as np
            
            img = cv2.cvtColor(np.array(page), cv2.COLOR_RGB2GRAY)
            # Increase contrast
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            img = clahe.apply(img)
            
            # Extract text with confidence
            data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
            text = pytesseract.image_to_string(img)
            
            full_text.append(text)
            
            # Calculate per-page confidence
            confidences = [int(c) for c in data['confidence'] if int(c) > 0]
            page_conf = sum(confidences) / len(confidences) if confidences else 0.0
            per_page_confidence.append(page_conf / 100.0)
        
        overall_confidence = sum(per_page_confidence) / len(per_page_confidence)
        
        return {
            'text': '\n\n'.join(full_text),
            'page_count': len(pages),
            'confidence': overall_confidence,
            'per_page_confidence': per_page_confidence,
            'model_used': 'tesseract-v5'
        }
    
    async def _extract_easyocr(self, pdf_path: str) -> dict:
        """Extract using EasyOCR (fallback, 93% accuracy)"""
        import easyocr
        from pdf2image import convert_from_path
        
        reader = easyocr.Reader(['en'], gpu=True)
        pages = convert_from_path(pdf_path)
        
        full_text = []
        per_page_confidence = []
        
        for page in pages:
            results = reader.readtext(page)
            
            text = '\n'.join([text for (bbox, text, confidence) in results])
            full_text.append(text)
            
            confidences = [conf for (bbox, text, conf) in results]
            page_conf = sum(confidences) / len(confidences) if confidences else 0.0
            per_page_confidence.append(page_conf)
        
        overall_confidence = sum(per_page_confidence) / len(per_page_confidence)
        
        return {
            'text': '\n\n'.join(full_text),
            'page_count': len(pages),
            'confidence': overall_confidence,
            'per_page_confidence': per_page_confidence,
            'model_used': 'easyocr'
        }
    
    async def batch_extract(self, pdfs: list, workers: int = 4) -> list:
        """
        Extract text from multiple PDFs in parallel
        
        Target: 25 PDFs (100 pages) in < 5 minutes
        """
        import asyncio
        
        semaphore = asyncio.Semaphore(workers)
        
        async def extract_with_semaphore(pdf_path):
            async with semaphore:
                return await self.extract_text(pdf_path)
        
        tasks = [extract_with_semaphore(pdf) for pdf in pdfs]
        results = await asyncio.gather(*tasks)
        
        return results

class DocumentContextExtractor:
    """Extract case-specific context from document"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    async def extract(self, text: str) -> dict:
        """
        Extract legal context from document text
        
        Returns:
            {
                'case_number': str,
                'parties': {'plaintiff': str, 'defendant': str},
                'relief_requested': str,
                'violations': [str],
                'key_dates': {'filing': date, 'hearing': date},
                ...
            }
        """
        import re
        from datetime import datetime
        
        context = {
            'case_number': self._extract_case_number(text),
            'parties': self._extract_parties(text),
            'relief_requested': self._extract_relief(text),
            'violations': self._extract_violations(text),
            'key_dates': self._extract_dates(text)
        }
        
        return context
    
    def _extract_case_number(self, text: str) -> str:
        """Extract case number (e.g., 2023-CV-12345)"""
        import re
        
        patterns = [
            r'CASE\s+(?:NO|NUMBER)[:\s]+([0-9]{4}-[A-Z]{2}-[0-9]+)',
            r'No\.\s+([0-9]{4}-[A-Z]{2}-[0-9]+)',
            r'Case\s+No\.\s+([0-9\-A-Z]+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return None
    
    def _extract_parties(self, text: str) -> dict:
        """Extract plaintiff and defendant"""
        parties = {'plaintiff': None, 'defendant': None}
        
        # Look for "v.", "vs.", "versus"
        import re
        
        patterns = [
            r'(\w+(?:\s+\w+)*?)\s+v\.\s+(\w+(?:\s+\w+)*?)(?:\n|,|$)',
            r'PLAINTIFF[:\s]+(.+?)(?:\n|$)',
            r'DEFENDANT[:\s]+(.+?)(?:\n|$)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                if isinstance(matches[0], tuple):
                    parties['plaintiff'] = matches[0][0]
                    parties['defendant'] = matches[0][1]
        
        return parties
    
    def _extract_relief(self, text: str) -> str:
        """Extract relief requested"""
        import re
        
        patterns = [
            r'relief\s+(?:requested|seeking)[:\s]+(.+?)(?:\n\n|$)',
            r'WHEREFORE[:\s]+(.+?)(?:\n\n|$)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                return match.group(1).strip()[:500]
        
        return None
    
    def _extract_violations(self, text: str) -> list:
        """Extract violation types"""
        violations = []
        
        keywords = {
            'breach_of_contract': ['breach', 'breached', 'failure to perform'],
            'fraud': ['fraud', 'fraudulent', 'misrepresentation'],
            'negligence': ['negligence', 'negligent', 'failed to'],
            'discrimination': ['discrimination', 'discriminatory', 'discriminated'],
            'breach_of_fiduciary': ['fiduciary', 'trust', 'breach of duty']
        }
        
        text_lower = text.lower()
        
        for violation_type, keywords_list in keywords.items():
            for keyword in keywords_list:
                if keyword in text_lower:
                    violations.append(violation_type)
                    break
        
        return violations
    
    def _extract_dates(self, text: str) -> dict:
        """Extract key dates"""
        import re
        from dateutil.parser import parse
        
        dates = {'filing': None, 'hearing': None, 'deadline': None}
        
        # Find all dates
        date_pattern = r'(\d{1,2}\/\d{1,2}\/\d{4}|\w+\s+\d{1,2},\s+\d{4})'
        matches = re.findall(date_pattern, text)
        
        for match in matches:
            try:
                date_obj = parse(match)
                
                if 'filing' in text[:text.find(match)].lower():
                    dates['filing'] = date_obj
                if 'hearing' in text[:text.find(match)].lower():
                    dates['hearing'] = date_obj
            except:
                pass
        
        return dates

class CaseKnowledgeGraphBuilder:
    """Build semantic knowledge graph from documents"""
    
    def __init__(self, vector_db=None):
        """
        Args:
            vector_db: Pinecone or Weaviate client
        """
        self.vector_db = vector_db
        self.logger = logging.getLogger(__name__)
    
    async def build_from_documents(self, case_id: int, documents: list) -> dict:
        """
        Build knowledge graph from case documents
        
        Returns:
            {
                'case_id': int,
                'num_entities': int,
                'num_relationships': int,
                'entity_types': {...},
                'graph_id': str
            }
        """
        # Step 1: Extract entities
        entities = await self._extract_entities(documents)
        
        # Step 2: Build relationships
        relationships = await self._build_relationships(entities, documents)
        
        # Step 3: Embed and index (if vector_db available)
        if self.vector_db:
            await self._index_graph_elements(case_id, entities, relationships)
        
        entity_type_counts = {}
        for entity in entities:
            entity_type = entity.get('type', 'unknown')
            entity_type_counts[entity_type] = entity_type_counts.get(entity_type, 0) + 1
        
        return {
            'case_id': case_id,
            'num_entities': len(entities),
            'num_relationships': len(relationships),
            'entity_types': entity_type_counts
        }
    
    async def _extract_entities(self, documents: list) -> list:
        """Extract entities using NER"""
        import spacy
        
        nlp = spacy.load("en_core_web_sm")
        
        entities = []
        
        for doc_text in documents:
            doc = nlp(doc_text[:10000])  # First 10K chars
            
            for ent in doc.ents:
                entity = {
                    'text': ent.text,
                    'type': ent.label_,
                    'confidence': 0.95  # spaCy confidence
                }
                
                # Avoid duplicates
                if entity not in entities:
                    entities.append(entity)
        
        return entities
    
    async def _build_relationships(self, entities: list, documents: list) -> list:
        """Build relationships between entities"""
        relationships = []
        
        # Simple relationship building: if two entities appear close together, they're related
        for doc_text in documents:
            for i, entity1 in enumerate(entities):
                for entity2 in entities[i+1:]:
                    
                    # Check if both entities appear in document
                    if entity1['text'] in doc_text and entity2['text'] in doc_text:
                        
                        # Check if close together (within 200 chars)
                        pos1 = doc_text.find(entity1['text'])
                        pos2 = doc_text.find(entity2['text'])
                        
                        if abs(pos1 - pos2) < 200:
                            relationships.append({
                                'entity1': entity1['text'],
                                'entity2': entity2['text'],
                                'relationship_type': 'related_in_document'
                            })
        
        return relationships
    
    async def _index_graph_elements(self, case_id: int, entities: list, relationships: list):
        """Index graph elements in vector DB"""
        # Implementation depends on vector DB choice
        pass

# ============================================================================
# BATCH PROCESSING ORCHESTRATOR
# ============================================================================

class BatchDocumentWorkflow:
    """End-to-end workflow: Load → OCR → Extract → Graph"""
    
    def __init__(self):
        self.pdf_loader = PDFBatchLoader()
        self.ocr_engine = OCREngine()
        self.context_extractor = DocumentContextExtractor()
        self.graph_builder = CaseKnowledgeGraphBuilder()
        self.logger = logging.getLogger(__name__)
    
    async def process_batch(self, pdf_files: list) -> dict:
        """
        End-to-end: Load 10 PDFs → OCR → Extract → Graph
        
        Returns:
            {
                'pdfs_loaded': int,
                'pdfs_processed': int,
                'errors': int,
                'total_pages': int,
                'knowledge_graph': {...},
                'duration_seconds': float
            }
        """
        import time
        
        start_time = time.time()
        
        result = {
            'pdfs_loaded': 0,
            'pdfs_processed': 0,
            'errors': 0,
            'total_pages': 0,
            'knowledge_graph': {'num_nodes': 0, 'num_edges': 0},
            'documents': []
        }
        
        # Step 1: Load PDFs
        self.logger.info(f"Loading {len(pdf_files)} PDFs...")
        load_results = await self.pdf_loader.load_batch(pdf_files)
        
        valid_pdfs = [r for r in load_results if r['status'] == 'loaded']
        result['pdfs_loaded'] = len(valid_pdfs)
        result['errors'] = len(load_results) - len(valid_pdfs)
        
        # Step 2: Extract text with OCR
        self.logger.info(f"Extracting text from {len(valid_pdfs)} PDFs...")
        ocr_results = await self.ocr_engine.batch_extract([r['file_path'] for r in valid_pdfs])
        
        result['pdfs_processed'] = len(ocr_results)
        result['total_pages'] = sum(r.get('page_count', 0) for r in ocr_results)
        
        # Step 3: Extract context from each document
        self.logger.info(f"Extracting context from documents...")
        all_contexts = []
        
        for i, ocr_result in enumerate(ocr_results):
            context = await self.context_extractor.extract(ocr_result['text'])
            all_contexts.append(context)
            
            result['documents'].append({
                'ocr_confidence': ocr_result.get('confidence', 0),
                'context': context
            })
        
        # Step 4: Build knowledge graph
        self.logger.info(f"Building knowledge graph...")
        texts = [r['text'] for r in ocr_results]
        graph_result = await self.graph_builder.build_from_documents(0, texts)
        
        result['knowledge_graph'] = {
            'num_nodes': graph_result.get('num_entities', 0),
            'num_edges': graph_result.get('num_relationships', 0),
            'entity_types': graph_result.get('entity_types', {})
        }
        
        result['duration_seconds'] = time.time() - start_time
        
        self.logger.info(f"Batch processing complete: {result}")
        
        return result

